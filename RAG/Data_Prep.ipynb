{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b030e78",
   "metadata": {},
   "source": [
    "## Data Prep for Multi-Modal RAG (Structured and Unstructured Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2bb21",
   "metadata": {},
   "source": [
    "![Data_Prep](./Assets/Data_Prep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f2fc7",
   "metadata": {},
   "source": [
    "### Installing Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.26.4 openai==1.69.0 langchain-community==0.4.1 PyPDF2==3.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bedad0",
   "metadata": {},
   "source": [
    "### Restarting Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51691c",
   "metadata": {},
   "source": [
    "### Saving Images and Docs in DBFS and Local File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_folder = \"./knowledge\"\n",
    "dbfs_target_folder = \"dbfs:/tmp/knowledge\"\n",
    "\n",
    "os.makedirs(source_folder, exist_ok=True)\n",
    "dbutils.fs.mkdirs(dbfs_target_folder)\n",
    "\n",
    "subfolders = ['docs', 'images']\n",
    "for subfolder in subfolders:\n",
    "    local_subfolder = os.path.join(source_folder, subfolder)\n",
    "    dbfs_subfolder = f\"{dbfs_target_folder}/{subfolder}\"\n",
    "    os.makedirs(local_subfolder, exist_ok=True)\n",
    "    dbutils.fs.mkdirs(dbfs_subfolder)\n",
    "    for filename in os.listdir(local_subfolder):\n",
    "        src = os.path.join(local_subfolder, filename)\n",
    "        dst = f\"{dbfs_subfolder}/{filename}\"\n",
    "        if os.path.isfile(src) and os.path.getsize(src) > 0:\n",
    "            dbutils.fs.cp(f\"file:{os.path.abspath(src)}\", dst, recurse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db98fbc",
   "metadata": {},
   "source": [
    "### Verfy Data is Stored in Local File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"./knowledge/\"))\n",
    "print(\" \\n images: \")\n",
    "print(os.listdir(\"./knowledge/images/\"))\n",
    "print(\" \\n docs: \")\n",
    "print(os.listdir(\"./knowledge/docs/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44106139",
   "metadata": {},
   "source": [
    "### Verify Data is Stored in DBFS mounted by ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4709947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbutils.fs.ls(\"dbfs:/tmp/knowledge/\"))\n",
    "print(\" \\n images: \")\n",
    "print(dbutils.fs.ls(\"dbfs:/tmp/knowledge/images/\"))\n",
    "print(\" \\n docs: \")\n",
    "print(dbutils.fs.ls(\"dbfs:/tmp/knowledge/docs/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbdfb3",
   "metadata": {},
   "source": [
    "### Creating the RAG Table Schema in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS YOUR_UNITY_CATALOG_NAME.RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66002184",
   "metadata": {},
   "source": [
    "### Creating an Image Table to Store Base64 Encoding of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = spark.read.format(\"binaryFile\").load(\"dbfs:/tmp/knowledge/images/\")\n",
    "images_df.createOrReplaceTempView(\"images_temp\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE RAG.images_metadata AS\n",
    "SELECT\n",
    "  path AS content_path,\n",
    "  base64(content) AS base64_content\n",
    "FROM images_temp\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"RAG.images_metadata\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d00bef",
   "metadata": {},
   "source": [
    "### Creating a Table with LLM Image Verbalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE RAG.images_verbalization AS\n",
    "SELECT\n",
    "  *,\n",
    "  ai_query(\n",
    "    'databricks-llama-4-maverick',\n",
    "    'what is this image about?', files => unbase64(base64_content)\n",
    "  ) AS chunk\n",
    "FROM RAG.images_metadata\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"RAG.images_verbalization\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b07c38",
   "metadata": {},
   "source": [
    "### Extracting PDF Content and Storing as Table with Langchain Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def perform_fixed_size_chunking(document, chunk_size=2000, chunk_overlap=500):\n",
    "    \"\"\"\n",
    "    Performs recursive chunking on a document with specified overlap.\n",
    "    Uses RecursiveCharacterTextSplitter which tries multiple separators.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(document)\n",
    "\n",
    "import os\n",
    "\n",
    "docs_folder = \"./knowledge/docs\"\n",
    "dbfs_docs_folder = \"dbfs:/tmp/knowledge/docs\"\n",
    "all_docs = []\n",
    "\n",
    "for filename in os.listdir(docs_folder):\n",
    "    file_path = os.path.join(docs_folder, filename)\n",
    "    dbfs_path = f\"{dbfs_docs_folder}/{filename}\"\n",
    "    if os.path.isfile(file_path) and filename.lower().endswith(\".pdf\"):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "            if text.strip():\n",
    "                chunks = perform_fixed_size_chunking(text)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if chunk.strip():\n",
    "                        all_docs.append({\n",
    "                            \"content_path\": dbfs_path,\n",
    "                            \"chunk\": chunk\n",
    "                        })\n",
    "\n",
    "if all_docs:\n",
    "    df = spark.createDataFrame(all_docs)\n",
    "    print(f\"Total chunks created: {df.count()}\")\n",
    "    print(f\"\\nChunks per document:\")\n",
    "    df.groupBy(\"content_path\").count().show(truncate=False)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No chunks extracted from documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"RAG.docs_chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0d630",
   "metadata": {},
   "source": [
    "### Creating the Final Multi-Modal RAG Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f95cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE RAG.final_rag_dataset AS\n",
    "SELECT monotonically_increasing_id() AS id, content_path, chunk FROM RAG.images_verbalization\n",
    "UNION ALL\n",
    "SELECT monotonically_increasing_id() AS id, content_path, chunk FROM RAG.docs_chunks\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"RAG.final_rag_dataset\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
