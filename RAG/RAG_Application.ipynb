{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b27570f",
   "metadata": {},
   "source": [
    "## Building a RAG Application with Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17a885",
   "metadata": {},
   "source": [
    "![rag_app](./Assets/rag_app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcbbd4",
   "metadata": {},
   "source": [
    "### Installing Utilities and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aff88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch==0.63 openai==1.69.0 mlflow==3.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da718a8",
   "metadata": {},
   "source": [
    "### Restarting our Python Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979af801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ec980",
   "metadata": {},
   "source": [
    "### Creating the RAG Model using MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d29000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import pyfunc\n",
    "from openai import OpenAI\n",
    "\n",
    "class RAGModel(pyfunc.PythonModel):\n",
    "    def __init__(self, vector_index):\n",
    "        self.vector_index = vector_index\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "          results_dict = self.vector_index.similarity_search(\n",
    "            query_text = query,\n",
    "            columns = [\"id\", \"content_path\", \"chunk\"],\n",
    "            num_results=10\n",
    "          )\n",
    "\n",
    "          return results_dict\n",
    "    \n",
    "    def chatCompletionsAPI(self, user_query, supporting_knowledge):\n",
    "        openai_client = OpenAI(\n",
    "            api_key = \"YOUR_DATABRICKS_ACCESS_TOKEN\",\n",
    "            base_url = \"YOUR_DATABRICKS_WORKSPACE_HOSTNAME/serving-endpoints\"\n",
    "        )\n",
    "        \n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model = \"databricks-claude-haiku-4-5\",\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You are a helpful assistant. You will be passed the user query and the supporting knowledge that can be used to answer the user_query\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"user query : {user_query} and supporting knowledge: {supporting_knowledge}\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def predict(self, context, data):\n",
    "          query = data[\"user_query\"].iloc[0]\n",
    "          text_data = self.retrieve(query)\n",
    "          return self.chatCompletionsAPI(query, text_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d049ebd",
   "metadata": {},
   "source": [
    "### Fetching our Vector Index with Mosaic AI Vector Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "vector_client = VectorSearchClient()\n",
    "\n",
    "# Use fully qualified index name: catalog.schema.index_name\n",
    "vector_index = vector_client.get_index(\n",
    "    index_name=\"YOUR_UNITY_CATALOG_NAME.rag.rag_vector_index\" # make sure this matches your vector index in Unity Catalog\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1b875",
   "metadata": {},
   "source": [
    "### Saving Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RAGModel(vector_index=vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "# Sample input\n",
    "input_example = pd.DataFrame([\n",
    "    {\"user_query\": \"Hi How are you?\"}\n",
    "])\n",
    "\n",
    "# Sample output (what your model actually returns)\n",
    "output_example = pd.DataFrame([\n",
    "    {\n",
    "        \"predictions\": \"I am good thank you!\"\n",
    "    }\n",
    "])\n",
    "\n",
    "# Infer full signature (input + output)\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "model_path = \"rag-model\"\n",
    "\n",
    "mlflow.pyfunc.save_model(path=model_path, python_model=test_model, signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccfabd",
   "metadata": {},
   "source": [
    "### Loading our Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our custom model from the local artifact store\n",
    "loaded_pyfunc_model = mlflow.pyfunc.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a36dce",
   "metadata": {},
   "source": [
    "### Testing our Loaded/Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.DataFrame([{\"user_query\": \"what is the carbonops ESG Intelligence Model? Give Citations too\"}])\n",
    "\n",
    "model_response = loaded_pyfunc_model.predict(model_input)\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71922126",
   "metadata": {},
   "source": [
    "### Logging our Saved Model as an Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "run_id = None\n",
    "\n",
    "# Log the model as an artifact\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_artifacts(local_dir=model_path, artifact_path=\"rag-model\")\n",
    "    print(f\"Model logged with run ID: {run.info.run_id}\")\n",
    "    run_id = run.info.run_id\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d2174",
   "metadata": {},
   "source": [
    "### Registering our RAG Model in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15601e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.register_model(f\"runs:/{run_id}/rag-model\", \"rag-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7391f",
   "metadata": {},
   "source": [
    "### Inferencing the Deployed Real-Time Endpoint\n",
    "use the below sample input when testing your endpoint from the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    "  \"dataframe_split\": {\n",
    "    \"columns\": [\n",
    "      \"user_query\"\n",
    "    ],\n",
    "    \"data\": [\n",
    "      [\n",
    "        \"tell me something about carbonops ESG intelligence model. Also state the citations\"\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
