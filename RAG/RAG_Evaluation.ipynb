{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e67b29",
   "metadata": {},
   "source": [
    "### Evaluating RAG Model with Databricks Evals and Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c8d76",
   "metadata": {},
   "source": [
    "![rag_eval](./Assets/rag_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1e7ee",
   "metadata": {},
   "source": [
    "### Installing Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch==0.63 openai==2.15.0 mlflow==3.8.1 databricks-agents==1.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9a22a",
   "metadata": {},
   "source": [
    "### Restarting our Python Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1283236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebde9a5",
   "metadata": {},
   "source": [
    "### Setting Up MLflow Tracing and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable MLflow's autologging to instrument your application with Tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Set up MLflow tracking to Databricks\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Users/YOUR_USER_ID_GOES_HERE/rag-app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdaec0",
   "metadata": {},
   "source": [
    "### Creating RAG Model with MLflow Tracing Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import pyfunc\n",
    "from openai import OpenAI\n",
    "\n",
    "class RAGModel(pyfunc.PythonModel):\n",
    "    def __init__(self, vector_index):\n",
    "        self.vector_index = vector_index\n",
    "    \n",
    "    @mlflow.trace(span_type=\"RETRIEVER\")\n",
    "    def retrieve(self, query):\n",
    "          results_dict = self.vector_index.similarity_search(\n",
    "            query_text = query,\n",
    "            columns = [\"id\", \"content_path\", \"chunk\"],\n",
    "            num_results=10\n",
    "          )\n",
    "\n",
    "          return results_dict\n",
    "    \n",
    "    @mlflow.trace\n",
    "    def chatCompletionsAPI(self, user_query, supporting_knowledge):\n",
    "        openai_client = OpenAI(\n",
    "            api_key = \"YOUR_DATABRICKS_ACCESS_TOKEN\",\n",
    "            base_url = \"YOUR_DATABRICKS_WORKSPACE_HOSTNAME/serving-endpoints\"\n",
    "        )\n",
    "        \n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model = \"databricks-claude-haiku-4-5\",\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You are a helpful assistant. You will be passed the user query and the supporting knowledge that can be used to answer the user_query\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"user query : {user_query} and supporting knowledge: {supporting_knowledge}\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def predict(self, context, data):\n",
    "          query = data[\"user_query\"].iloc[0]\n",
    "          text_data = self.retrieve(query)\n",
    "          return self.chatCompletionsAPI(query, text_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef05ff",
   "metadata": {},
   "source": [
    "### Fetching our Mosaic AI Vector Index from Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d08d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "vector_client = VectorSearchClient()\n",
    "\n",
    "# Use fully qualified index name: catalog.schema.index_name\n",
    "vector_index = vector_client.get_index(\n",
    "    index_name=\"YOUR_UNITY_CATALOG_NAME.rag.rag_vector_index\" # make sure this matches your vector index in Unity Catalog\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33f276",
   "metadata": {},
   "source": [
    "### Saving our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RAGModel(vector_index=vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "# Sample input\n",
    "input_example = pd.DataFrame([\n",
    "    {\"user_query\": \"Hi How are you?\"}\n",
    "])\n",
    "\n",
    "# Sample output (what your model actually returns)\n",
    "output_example = pd.DataFrame([\n",
    "    {\n",
    "        \"predictions\": \"I am good thank you!\"\n",
    "    }\n",
    "])\n",
    "\n",
    "# Infer full signature (input + output)\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "model_path = \"rag-model-experimentation\"\n",
    "\n",
    "mlflow.pyfunc.save_model(path=model_path, python_model=test_model, signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c62a3",
   "metadata": {},
   "source": [
    "### Loading our Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eee1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our custom model from the local artifact store\n",
    "loaded_pyfunc_model = mlflow.pyfunc.load_model(\"rag-model-experimentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe091c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.DataFrame([{\"user_query\": \"what is the carbonops ESG Intelligence Model? Give Citations too\"}])\n",
    "\n",
    "model_response = loaded_pyfunc_model.predict(model_input)\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473a04",
   "metadata": {},
   "source": [
    "### Simulating Production Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86056972",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_requests = [\n",
    "    {\"user_query\": \"Tell me something about CarbonOps Global Remote Work Policy\"},\n",
    "    {\"user_query\": \"Tell me something about CarbonOps Sustainable Development Goals Mapper\"},\n",
    "    {\"user_query\": \"How does CarbonOps MARK framework help in ESG materiality Analysis\"},\n",
    "    {\"user_query\": \"Comment on the Work Culture at CarbonOps\"},\n",
    "    {\"user_query\": \"Explain Materiality Dimensions in European ESRS Reporting\"}\n",
    "]\n",
    "\n",
    "# RUn requets and capture traces\n",
    "print(\"Simulating production traffic........\")\n",
    "for req in test_requests:\n",
    "    try:\n",
    "        result = loaded_pyfunc_model.predict(pd.DataFrame([req]))\n",
    "        print(f\"Question: {req['user_query']} \\n\")\n",
    "        print(f\"Answer: {result} \\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"encountered error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f25f51",
   "metadata": {},
   "source": [
    "### Running Evaluation using MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    RetrievalGroundedness,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.genai import datasets\n",
    "\n",
    "EVAL_DATASET_NAME='YOUR_UNITY_CATALOG_NAME.SCHEMA.TABLE'\n",
    "\n",
    "# Sync data to unity catalog.\n",
    "eval_dataset = datasets.get_dataset(EVAL_DATASET_NAME)\n",
    "\n",
    "# Create a wrapper function that matches the dataset input structure\n",
    "def predict_wrapper(user_query, supporting_knowledge=None):\n",
    "    \"\"\"Wrapper to convert dataset inputs to model's expected format\"\"\"\n",
    "    input_df = pd.DataFrame([{\"user_query\": user_query}])\n",
    "    return loaded_pyfunc_model.predict(input_df)\n",
    "\n",
    "eval_judges = [\n",
    "    Guidelines(\n",
    "        name=\"Conciseness\",\n",
    "        guidelines=\"The response should be concise and to the point\"\n",
    "    ),\n",
    "    RelevanceToQuery(),\n",
    "    Safety(),\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data = eval_dataset,\n",
    "    predict_fn=predict_wrapper,\n",
    "    scorers = eval_judges\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
